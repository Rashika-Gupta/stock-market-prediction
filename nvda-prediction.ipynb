{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhythm/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market = pd.read_excel('./nvda-stockmarket.xlsx',engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 353 entries, 0 to 352\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   Contract Name          353 non-null    object        \n",
      " 1   Last Trade Date (EDT)  353 non-null    datetime64[ns]\n",
      " 2   Strike                 353 non-null    float64       \n",
      " 3   Last Price             353 non-null    float64       \n",
      " 4   Bid                    353 non-null    float64       \n",
      " 5   Ask                    353 non-null    float64       \n",
      " 6   Change                 353 non-null    float64       \n",
      " 7   % Change               353 non-null    float64       \n",
      " 8   Volume                 353 non-null    object        \n",
      " 9   Open Interest          353 non-null    int64         \n",
      " 10  Implied Volatility     353 non-null    float64       \n",
      " 11  Option Type            353 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(7), int64(1), object(3)\n",
      "memory usage: 33.2+ KB\n"
     ]
    }
   ],
   "source": [
    "stock_market.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = {\n",
    "    '% Change': 'dChange',\n",
    "    \n",
    "}\n",
    "\n",
    "stock_market.rename(columns=new_column_names, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock market date time to date time\n",
    "stock_market['Date'] = stock_market['Last Trade Date (EDT)'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market['Time'] = stock_market['Last Trade Date (EDT)'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contract Name</th>\n",
       "      <th>Last Trade Date (EDT)</th>\n",
       "      <th>Strike</th>\n",
       "      <th>Last Price</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Change</th>\n",
       "      <th>dChange</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open Interest</th>\n",
       "      <th>Implied Volatility</th>\n",
       "      <th>Option Type</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NVDA240301C00250000</td>\n",
       "      <td>2024-02-27 20:31:00</td>\n",
       "      <td>250.0</td>\n",
       "      <td>540.03</td>\n",
       "      <td>533.55</td>\n",
       "      <td>535.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>6.1533</td>\n",
       "      <td>call</td>\n",
       "      <td>2024-02-27</td>\n",
       "      <td>20:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVDA240301C00260000</td>\n",
       "      <td>2024-02-26 20:59:00</td>\n",
       "      <td>260.0</td>\n",
       "      <td>531.66</td>\n",
       "      <td>523.95</td>\n",
       "      <td>525.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>6.0527</td>\n",
       "      <td>call</td>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>20:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NVDA240301C00270000</td>\n",
       "      <td>2024-02-26 20:13:00</td>\n",
       "      <td>270.0</td>\n",
       "      <td>523.00</td>\n",
       "      <td>513.55</td>\n",
       "      <td>515.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>5.7617</td>\n",
       "      <td>call</td>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>20:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NVDA240301C00280000</td>\n",
       "      <td>2024-02-22 18:03:00</td>\n",
       "      <td>280.0</td>\n",
       "      <td>495.19</td>\n",
       "      <td>504.00</td>\n",
       "      <td>505.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>call</td>\n",
       "      <td>2024-02-22</td>\n",
       "      <td>18:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NVDA240301C00290000</td>\n",
       "      <td>2024-02-26 20:02:00</td>\n",
       "      <td>290.0</td>\n",
       "      <td>500.59</td>\n",
       "      <td>494.15</td>\n",
       "      <td>495.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5967</td>\n",
       "      <td>call</td>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>20:02:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Contract Name Last Trade Date (EDT)  Strike  Last Price     Bid  \\\n",
       "0  NVDA240301C00250000   2024-02-27 20:31:00   250.0      540.03  533.55   \n",
       "1  NVDA240301C00260000   2024-02-26 20:59:00   260.0      531.66  523.95   \n",
       "2  NVDA240301C00270000   2024-02-26 20:13:00   270.0      523.00  513.55   \n",
       "3  NVDA240301C00280000   2024-02-22 18:03:00   280.0      495.19  504.00   \n",
       "4  NVDA240301C00290000   2024-02-26 20:02:00   290.0      500.59  494.15   \n",
       "\n",
       "     Ask  Change  dChange Volume  Open Interest  Implied Volatility  \\\n",
       "0  535.4     0.0      0.0      9             31              6.1533   \n",
       "1  525.4     0.0      0.0      3             12              6.0527   \n",
       "2  515.4     0.0      0.0     28             16              5.7617   \n",
       "3  505.8     0.0      0.0      1             16              5.7690   \n",
       "4  495.7     0.0      0.0     30              2              5.5967   \n",
       "\n",
       "  Option Type        Date      Time  \n",
       "0        call  2024-02-27  20:31:00  \n",
       "1        call  2024-02-26  20:59:00  \n",
       "2        call  2024-02-26  20:13:00  \n",
       "3        call  2024-02-22  18:03:00  \n",
       "4        call  2024-02-26  20:02:00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_market.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market = stock_market.drop(['Contract Name','Last Trade Date (EDT)','Date', 'Time'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntCastingNaNError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-101505e565e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert to integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstock_market\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Volume'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_market\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Volume'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5813\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5814\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5815\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5816\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     def convert(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mastype_float_to_int_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_float_to_int_nansafe\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m         raise IntCastingNaNError(\n\u001b[0;32m-> 1214\u001b[0;31m             \u001b[0;34m\"Cannot convert non-finite values (NA or inf) to integer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m         )\n\u001b[1;32m   1216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIntCastingNaNError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "# Replace '-' with NaN and convert to numeric\n",
    "stock_market['Volume'] = pd.to_numeric(stock_market['Volume'], errors='coerce')\n",
    "\n",
    "# Drop NaN values if needed\n",
    "# df.dropna(subset=['Volume'], inplace=True)\n",
    "\n",
    "# Convert to integer\n",
    "stock_market['Volume'] = stock_market['Volume'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4-54 nAN values in volume, Let's get rid of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_market['Volume'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market = stock_market.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 349 entries, 0 to 352\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Strike              349 non-null    float64\n",
      " 1   Last Price          349 non-null    float64\n",
      " 2   Bid                 349 non-null    float64\n",
      " 3   Ask                 349 non-null    float64\n",
      " 4   Change              349 non-null    float64\n",
      " 5   dChange             349 non-null    float64\n",
      " 6   Volume              349 non-null    float64\n",
      " 7   Open Interest       349 non-null    int64  \n",
      " 8   Implied Volatility  349 non-null    float64\n",
      " 9   Option Type         349 non-null    object \n",
      "dtypes: float64(8), int64(1), object(1)\n",
      "memory usage: 30.0+ KB\n"
     ]
    }
   ],
   "source": [
    "stock_market.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking for NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Contract Name         0\n",
       "Strike                0\n",
       "Last Price            0\n",
       "Bid                   0\n",
       "Ask                   0\n",
       "Change                0\n",
       "% Change              0\n",
       "Volume                0\n",
       "Open Interest         0\n",
       "Implied Volatility    0\n",
       "Option Type           0\n",
       "Date                  0\n",
       "Time                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockMarket.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strike</th>\n",
       "      <th>Last Price</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Change</th>\n",
       "      <th>% Change</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open Interest</th>\n",
       "      <th>Implied Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>353.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>349.000000</td>\n",
       "      <td>353.000000</td>\n",
       "      <td>353.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>725.750708</td>\n",
       "      <td>110.142096</td>\n",
       "      <td>111.235467</td>\n",
       "      <td>112.598244</td>\n",
       "      <td>-1.214136</td>\n",
       "      <td>-0.053412</td>\n",
       "      <td>1142.690544</td>\n",
       "      <td>1511.569405</td>\n",
       "      <td>1.443579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.239402</td>\n",
       "      <td>153.369613</td>\n",
       "      <td>154.211363</td>\n",
       "      <td>155.587459</td>\n",
       "      <td>4.186927</td>\n",
       "      <td>0.237244</td>\n",
       "      <td>3772.058173</td>\n",
       "      <td>2520.052099</td>\n",
       "      <td>1.221472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-20.890000</td>\n",
       "      <td>-0.542700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>515.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.078300</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>700.000000</td>\n",
       "      <td>6.450000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>6.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>1.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>910.000000</td>\n",
       "      <td>203.830000</td>\n",
       "      <td>201.100000</td>\n",
       "      <td>204.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1.869100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1300.000000</td>\n",
       "      <td>540.030000</td>\n",
       "      <td>533.550000</td>\n",
       "      <td>535.400000</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>31663.000000</td>\n",
       "      <td>18384.000000</td>\n",
       "      <td>6.153300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Strike  Last Price         Bid         Ask      Change  \\\n",
       "count   353.000000  353.000000  353.000000  353.000000  353.000000   \n",
       "mean    725.750708  110.142096  111.235467  112.598244   -1.214136   \n",
       "std     266.239402  153.369613  154.211363  155.587459    4.186927   \n",
       "min     250.000000    0.010000    0.000000    0.000000  -20.890000   \n",
       "25%     515.000000    0.020000    0.010000    0.020000   -0.200000   \n",
       "50%     700.000000    6.450000    6.400000    6.550000    0.000000   \n",
       "75%     910.000000  203.830000  201.100000  204.850000    0.000000   \n",
       "max    1300.000000  540.030000  533.550000  535.400000   19.750000   \n",
       "\n",
       "         % Change        Volume  Open Interest  Implied Volatility  \n",
       "count  353.000000    349.000000     353.000000          353.000000  \n",
       "mean    -0.053412   1142.690544    1511.569405            1.443579  \n",
       "std      0.237244   3772.058173    2520.052099            1.221472  \n",
       "min     -0.542700      1.000000       0.000000            0.000000  \n",
       "25%     -0.078300      3.000000      31.000000            0.599600  \n",
       "50%      0.000000     20.000000     526.000000            1.062500  \n",
       "75%      0.000000    254.000000    1872.000000            1.869100  \n",
       "max      2.000000  31663.000000   18384.000000            6.153300  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of calls and Puts : call    180\n",
      "Puts    169\n",
      "Name: Option Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of calls and Puts : {stock_market[\"Option Type\"].value_counts()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if buying a call then betting that it will go up\n",
    "if put then will go down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trades made for March 1 as time of expiration  : Date\n",
      "2024-01-25      2\n",
      "2024-01-26      1\n",
      "2024-01-29      1\n",
      "2024-02-01      1\n",
      "2024-02-06      1\n",
      "2024-02-07      1\n",
      "2024-02-14      3\n",
      "2024-02-16      1\n",
      "2024-02-20      3\n",
      "2024-02-21      1\n",
      "2024-02-22     28\n",
      "2024-02-23     11\n",
      "2024-02-26     47\n",
      "2024-02-27     43\n",
      "2024-02-28    209\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of trades made for March 1 as time of expiration  : {stockMarket[\"Date\"].value_counts().sort_index(ascending=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "date_counts_increasing = stockMarket[\"Date\"].value_counts().sort_index(ascending=True)\n",
    "\n",
    "# Now, date_counts_increasing.index and date_counts_increasing.values can be obtained as follows:\n",
    "date_index = date_counts_increasing.index\n",
    "counts_values = date_counts_increasing.values\n",
    "\n",
    "# Assuming you have date_counts_increasing or date_counts_decreasing as a pandas Series\n",
    "fig = px.bar(x=date_counts_increasing.index, y=date_counts_increasing.values,\n",
    "             labels={'x': 'Date', 'y': 'Number of Trades'},\n",
    "             title='Number of Trades by Date (Increasing Order)')\n",
    "\n",
    "# To display the plot as an interactive HTML file\n",
    "fig.write_html('trades_by_date_increasing.html')\n",
    "\n",
    "# To display the plot in a Jupyter notebook (if using Jupyter)\n",
    "#fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Convert 'Option Type' to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "stock_market['Option Type'] = label_encoder.fit_transform(stock_market['Option Type'])\n",
    "\n",
    "# Separate features and target\n",
    "y = stock_market['Option Type']\n",
    "X = stock_market.drop('Option Type', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 349 entries, 0 to 352\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Strike              349 non-null    float64\n",
      " 1   Last Price          349 non-null    float64\n",
      " 2   Bid                 349 non-null    float64\n",
      " 3   Ask                 349 non-null    float64\n",
      " 4   Change              349 non-null    float64\n",
      " 5   dChange             349 non-null    float64\n",
      " 6   Volume              349 non-null    float64\n",
      " 7   Open Interest       349 non-null    int64  \n",
      " 8   Implied Volatility  349 non-null    float64\n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 27.3 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((349, 9), (349,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turninig Data to Tensor \n",
    "X = torch.from_numpy(X.values).type(torch.float)\n",
    "y = torch.from_numpy(y.values).type(torch.float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. input layer shape = # of features\n",
    "2. Output Layer shape = 1(one class or the other) for multiclassificatin = 3 \n",
    "3. Hidden Layer Activation usually ReLu could use other too\n",
    "4. Output activation Sigmoid, Softmax for multiclass\n",
    "5. Loss Function Binary CrossEntropy\n",
    "6. Optimizer = SGD, Adan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-bd629803b521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (layer_1): Linear(in_features=9, out_features=6, bias=True)\n",
       "  (layer_2): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the neural network model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=9, out_features=6) # takes in 11 features (X), produces 6 features\n",
    "        self.layer_2 = nn.Linear(in_features=6, out_features=1) # takes in 6 features, produces 1 feature (y)\n",
    "    # 3. Define a forward method containing the forward pass computation\n",
    "    def forward(self, x):\n",
    "        # Return the output of layer_2, a single feature, the same shape as y\n",
    "        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n",
    "\n",
    "# 4. Create an instance of the model and send it to target device\n",
    "model_0 = BinaryClassifier()\n",
    "model_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of predictions: 70, Shape: torch.Size([70, 1])\n",
      "Length of test samples: 70, Shape: torch.Size([70])\n",
      "\n",
      "First 10 predictions:\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "First 10 test labels:\n",
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Make predictions with the model\n",
    "untrained_preds = model_0(X_test)\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=deep.parameters(), \n",
    "                            lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor does not contain NaN values.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "has_nan = torch.isnan(X_train).any().item()\n",
    "if has_nan:\n",
    "    print(\"The tensor contains NaN values.\")\n",
    "else:\n",
    "    print(\"The tensor does not contain NaN values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 143.59129 | Test loss: 341.18546\n",
      "Epoch: 1 | Loss: 242.75963 | Test loss: 29.29285\n",
      "Epoch: 2 | Loss: 19.02667 | Test loss: 18.66355\n",
      "Epoch: 3 | Loss: 13.87927 | Test loss: 57.40464\n",
      "Epoch: 4 | Loss: 35.56451 | Test loss: 11.54196\n",
      "Epoch: 5 | Loss: 6.53236 | Test loss: 28.11400\n",
      "Epoch: 6 | Loss: 19.98043 | Test loss: 27.34684\n",
      "Epoch: 7 | Loss: 17.10603 | Test loss: 2.74864\n",
      "Epoch: 8 | Loss: 2.06041 | Test loss: 18.74428\n",
      "Epoch: 9 | Loss: 11.44220 | Test loss: 11.97199\n",
      "Epoch: 10 | Loss: 7.33447 | Test loss: 2.18331\n",
      "Epoch: 11 | Loss: 1.85456 | Test loss: 11.59234\n",
      "Epoch: 12 | Loss: 7.02199 | Test loss: 4.17031\n",
      "Epoch: 13 | Loss: 2.74874 | Test loss: 15.72265\n",
      "Epoch: 14 | Loss: 13.79070 | Test loss: 13.57867\n",
      "Epoch: 15 | Loss: 9.61518 | Test loss: 1.98614\n",
      "Epoch: 16 | Loss: 2.04755 | Test loss: 13.15585\n",
      "Epoch: 17 | Loss: 9.90176 | Test loss: 1.32624\n",
      "Epoch: 18 | Loss: 1.16874 | Test loss: 7.48221\n",
      "Epoch: 19 | Loss: 6.72798 | Test loss: 7.49547\n",
      "Epoch: 20 | Loss: 5.62586 | Test loss: 1.69656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Loss: 1.50288 | Test loss: 6.06568\n",
      "Epoch: 22 | Loss: 4.31421 | Test loss: 3.06850\n",
      "Epoch: 23 | Loss: 2.46807 | Test loss: 4.21129\n",
      "Epoch: 24 | Loss: 3.54466 | Test loss: 3.75095\n",
      "Epoch: 25 | Loss: 3.18610 | Test loss: 5.43813\n",
      "Epoch: 26 | Loss: 4.43060 | Test loss: 1.05594\n",
      "Epoch: 27 | Loss: 1.01376 | Test loss: 1.16590\n",
      "Epoch: 28 | Loss: 0.98542 | Test loss: 1.51749\n",
      "Epoch: 29 | Loss: 1.10727 | Test loss: 1.46089\n",
      "Epoch: 30 | Loss: 1.48498 | Test loss: 2.48416\n",
      "Epoch: 31 | Loss: 2.37024 | Test loss: 4.04444\n",
      "Epoch: 32 | Loss: 3.27785 | Test loss: 0.72790\n",
      "Epoch: 33 | Loss: 0.80031 | Test loss: 2.22055\n",
      "Epoch: 34 | Loss: 1.75533 | Test loss: 1.56449\n",
      "Epoch: 35 | Loss: 1.58086 | Test loss: 3.21894\n",
      "Epoch: 36 | Loss: 2.65339 | Test loss: 0.94416\n",
      "Epoch: 37 | Loss: 0.86329 | Test loss: 1.32623\n",
      "Epoch: 38 | Loss: 1.26197 | Test loss: 2.19965\n",
      "Epoch: 39 | Loss: 1.94935 | Test loss: 0.75794\n",
      "Epoch: 40 | Loss: 0.75120 | Test loss: 0.99759\n",
      "Epoch: 41 | Loss: 0.95803 | Test loss: 1.65659\n",
      "Epoch: 42 | Loss: 1.58623 | Test loss: 0.98052\n",
      "Epoch: 43 | Loss: 0.92379 | Test loss: 1.40960\n",
      "Epoch: 44 | Loss: 1.36824 | Test loss: 2.54347\n",
      "Epoch: 45 | Loss: 2.17745 | Test loss: 1.70306\n",
      "Epoch: 46 | Loss: 1.67022 | Test loss: 0.92062\n",
      "Epoch: 47 | Loss: 0.95702 | Test loss: 0.96861\n",
      "Epoch: 48 | Loss: 0.92867 | Test loss: 0.87557\n",
      "Epoch: 49 | Loss: 0.84008 | Test loss: 0.79732\n",
      "Epoch: 50 | Loss: 0.76995 | Test loss: 0.84529\n",
      "Epoch: 51 | Loss: 0.80020 | Test loss: 0.73879\n",
      "Epoch: 52 | Loss: 0.73869 | Test loss: 0.78300\n",
      "Epoch: 53 | Loss: 0.76905 | Test loss: 0.96489\n",
      "Epoch: 54 | Loss: 0.93468 | Test loss: 1.33041\n",
      "Epoch: 55 | Loss: 1.22556 | Test loss: 1.53282\n",
      "Epoch: 56 | Loss: 1.49325 | Test loss: 0.74070\n",
      "Epoch: 57 | Loss: 0.78170 | Test loss: 0.81760\n",
      "Epoch: 58 | Loss: 0.78611 | Test loss: 1.01372\n",
      "Epoch: 59 | Loss: 1.02583 | Test loss: 0.76913\n",
      "Epoch: 60 | Loss: 0.76063 | Test loss: 1.06340\n",
      "Epoch: 61 | Loss: 1.05591 | Test loss: 0.79079\n",
      "Epoch: 62 | Loss: 0.77483 | Test loss: 1.03316\n",
      "Epoch: 63 | Loss: 1.04193 | Test loss: 0.82114\n",
      "Epoch: 64 | Loss: 0.78826 | Test loss: 0.99899\n",
      "Epoch: 65 | Loss: 1.02426 | Test loss: 0.78483\n",
      "Epoch: 66 | Loss: 0.76732 | Test loss: 0.94891\n",
      "Epoch: 67 | Loss: 0.96367 | Test loss: 0.80371\n",
      "Epoch: 68 | Loss: 0.78052 | Test loss: 0.98845\n",
      "Epoch: 69 | Loss: 1.00460 | Test loss: 0.73390\n",
      "Epoch: 70 | Loss: 0.71874 | Test loss: 0.82580\n",
      "Epoch: 71 | Loss: 0.85065 | Test loss: 0.84142\n",
      "Epoch: 72 | Loss: 0.80652 | Test loss: 1.02767\n",
      "Epoch: 73 | Loss: 1.05049 | Test loss: 0.67578\n",
      "Epoch: 74 | Loss: 0.67859 | Test loss: 0.68359\n",
      "Epoch: 75 | Loss: 0.69710 | Test loss: 0.79590\n",
      "Epoch: 76 | Loss: 0.76672 | Test loss: 0.95828\n",
      "Epoch: 77 | Loss: 0.98542 | Test loss: 0.70294\n",
      "Epoch: 78 | Loss: 0.69485 | Test loss: 0.75767\n",
      "Epoch: 79 | Loss: 0.78020 | Test loss: 0.85265\n",
      "Epoch: 80 | Loss: 0.81231 | Test loss: 1.00946\n",
      "Epoch: 81 | Loss: 1.04045 | Test loss: 0.67749\n",
      "Epoch: 82 | Loss: 0.67840 | Test loss: 0.67832\n",
      "Epoch: 83 | Loss: 0.69064 | Test loss: 0.75941\n",
      "Epoch: 84 | Loss: 0.73587 | Test loss: 0.87637\n",
      "Epoch: 85 | Loss: 0.90534 | Test loss: 0.75434\n",
      "Epoch: 86 | Loss: 0.73246 | Test loss: 0.86644\n",
      "Epoch: 87 | Loss: 0.89322 | Test loss: 0.75998\n",
      "Epoch: 88 | Loss: 0.73745 | Test loss: 0.87445\n",
      "Epoch: 89 | Loss: 0.90181 | Test loss: 0.75029\n",
      "Epoch: 90 | Loss: 0.72991 | Test loss: 0.85037\n",
      "Epoch: 91 | Loss: 0.87963 | Test loss: 0.76694\n",
      "Epoch: 92 | Loss: 0.74169 | Test loss: 0.87272\n",
      "Epoch: 93 | Loss: 0.90414 | Test loss: 0.74708\n",
      "Epoch: 94 | Loss: 0.72574 | Test loss: 0.83414\n",
      "Epoch: 95 | Loss: 0.86170 | Test loss: 0.77623\n",
      "Epoch: 96 | Loss: 0.74862 | Test loss: 0.88547\n",
      "Epoch: 97 | Loss: 0.91270 | Test loss: 0.73465\n",
      "Epoch: 98 | Loss: 0.71776 | Test loss: 0.80775\n",
      "Epoch: 99 | Loss: 0.83371 | Test loss: 0.78917\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = model_0(X_train).squeeze()\n",
    "    # Squeeze to remove extra `1` dimensions; this won't work unless model and data are on the same device \n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))  # Turn logits -> pred probs -> pred labels\n",
    "\n",
    "    # 2. Calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train) \n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_0(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        \n",
    "        # 2. Calculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "    \n",
    "    # Print out what's happening every epoch\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss.item():.5f} | Test loss: {test_loss.item():.5f}\")\n",
    "\n",
    "     # Print intermediate values for debugging\n",
    "    #print(\"y_pred:\", y_pred)\n",
    "    #print(\"y_logits:\", y_logits)\n",
    "    #print(\"test_pred:\", test_pred)\n",
    "    #print(\"test_logits:\", test_logits)\n",
    "\n",
    "    # Check for NaN or infinite values in tensors\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(\"Training loss is NaN or infinite. Exiting training loop.\")\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changing the model and making it denser\n",
    "2. Plot the losses and. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DeeperBinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeeperBinaryClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=9, out_features=12)\n",
    "        self.layer_2 = nn.Linear(in_features=12, out_features=8)\n",
    "        self.layer_3 = nn.Linear(in_features=8, out_features=4)\n",
    "        self.layer_4 = nn.Linear(in_features=4, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = F.relu(self.layer_3(x))\n",
    "        x = self.layer_4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeperBinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.15516 | Test loss: 0.68678\n",
      "Epoch: 1 | Loss: 0.69333 | Test loss: 0.68678\n",
      "Epoch: 2 | Loss: 0.69402 | Test loss: 0.68678\n",
      "Epoch: 3 | Loss: 0.69365 | Test loss: 0.68678\n",
      "Epoch: 4 | Loss: 0.69385 | Test loss: 0.68679\n",
      "Epoch: 5 | Loss: 0.69363 | Test loss: 0.68679\n",
      "Epoch: 6 | Loss: 0.69379 | Test loss: 0.68679\n",
      "Epoch: 7 | Loss: 0.69359 | Test loss: 0.68679\n",
      "Epoch: 8 | Loss: 0.69373 | Test loss: 0.68679\n",
      "Epoch: 9 | Loss: 0.69354 | Test loss: 0.68679\n",
      "Epoch: 10 | Loss: 0.69368 | Test loss: 0.68679\n",
      "Epoch: 11 | Loss: 0.69350 | Test loss: 0.68679\n",
      "Epoch: 12 | Loss: 0.69363 | Test loss: 0.68679\n",
      "Epoch: 13 | Loss: 0.69347 | Test loss: 0.68680\n",
      "Epoch: 14 | Loss: 0.69357 | Test loss: 0.68680\n",
      "Epoch: 15 | Loss: 0.69343 | Test loss: 0.68680\n",
      "Epoch: 16 | Loss: 0.69352 | Test loss: 0.68680\n",
      "Epoch: 17 | Loss: 0.69339 | Test loss: 0.68680\n",
      "Epoch: 18 | Loss: 0.69347 | Test loss: 0.68680\n",
      "Epoch: 19 | Loss: 0.69335 | Test loss: 0.68680\n",
      "Epoch: 20 | Loss: 0.69342 | Test loss: 0.68680\n",
      "Epoch: 21 | Loss: 0.69331 | Test loss: 0.68680\n",
      "Epoch: 22 | Loss: 0.69337 | Test loss: 0.68681\n",
      "Epoch: 23 | Loss: 0.69328 | Test loss: 0.68681\n",
      "Epoch: 24 | Loss: 0.69332 | Test loss: 0.68681\n",
      "Epoch: 25 | Loss: 0.69324 | Test loss: 0.68681\n",
      "Epoch: 26 | Loss: 0.69327 | Test loss: 0.68681\n",
      "Epoch: 27 | Loss: 0.69320 | Test loss: 0.68681\n",
      "Epoch: 28 | Loss: 0.69322 | Test loss: 0.68681\n",
      "Epoch: 29 | Loss: 0.69317 | Test loss: 0.68681\n",
      "Epoch: 30 | Loss: 0.69317 | Test loss: 0.68682\n",
      "Epoch: 31 | Loss: 0.69314 | Test loss: 0.68682\n",
      "Epoch: 32 | Loss: 0.69313 | Test loss: 0.68682\n",
      "Epoch: 33 | Loss: 0.69310 | Test loss: 0.68682\n",
      "Epoch: 34 | Loss: 0.69308 | Test loss: 0.68682\n",
      "Epoch: 35 | Loss: 0.69307 | Test loss: 0.68682\n",
      "Epoch: 36 | Loss: 0.69304 | Test loss: 0.68682\n",
      "Epoch: 37 | Loss: 0.69303 | Test loss: 0.68682\n",
      "Epoch: 38 | Loss: 0.69299 | Test loss: 0.68682\n",
      "Epoch: 39 | Loss: 0.69300 | Test loss: 0.68683\n",
      "Epoch: 40 | Loss: 0.69294 | Test loss: 0.68683\n",
      "Epoch: 41 | Loss: 0.69297 | Test loss: 0.68683\n",
      "Epoch: 42 | Loss: 0.69290 | Test loss: 0.68683\n",
      "Epoch: 43 | Loss: 0.69294 | Test loss: 0.68683\n",
      "Epoch: 44 | Loss: 0.69286 | Test loss: 0.68683\n",
      "Epoch: 45 | Loss: 0.69291 | Test loss: 0.68683\n",
      "Epoch: 46 | Loss: 0.69281 | Test loss: 0.68683\n",
      "Epoch: 47 | Loss: 0.69288 | Test loss: 0.68684\n",
      "Epoch: 48 | Loss: 0.69277 | Test loss: 0.68684\n",
      "Epoch: 49 | Loss: 0.69285 | Test loss: 0.68684\n",
      "Epoch: 50 | Loss: 0.69273 | Test loss: 0.68684\n",
      "Epoch: 51 | Loss: 0.69282 | Test loss: 0.68684\n",
      "Epoch: 52 | Loss: 0.69269 | Test loss: 0.68684\n",
      "Epoch: 53 | Loss: 0.69279 | Test loss: 0.68684\n",
      "Epoch: 54 | Loss: 0.69265 | Test loss: 0.68684\n",
      "Epoch: 55 | Loss: 0.69276 | Test loss: 0.68684\n",
      "Epoch: 56 | Loss: 0.69261 | Test loss: 0.68685\n",
      "Epoch: 57 | Loss: 0.69273 | Test loss: 0.68685\n",
      "Epoch: 58 | Loss: 0.69256 | Test loss: 0.68685\n",
      "Epoch: 59 | Loss: 0.69270 | Test loss: 0.68685\n",
      "Epoch: 60 | Loss: 0.69253 | Test loss: 0.68685\n",
      "Epoch: 61 | Loss: 0.69267 | Test loss: 0.68685\n",
      "Epoch: 62 | Loss: 0.69249 | Test loss: 0.68685\n",
      "Epoch: 63 | Loss: 0.69264 | Test loss: 0.68685\n",
      "Epoch: 64 | Loss: 0.69245 | Test loss: 0.68685\n",
      "Epoch: 65 | Loss: 0.69262 | Test loss: 0.68686\n",
      "Epoch: 66 | Loss: 0.69241 | Test loss: 0.68686\n",
      "Epoch: 67 | Loss: 0.69259 | Test loss: 0.68686\n",
      "Epoch: 68 | Loss: 0.69237 | Test loss: 0.68686\n",
      "Epoch: 69 | Loss: 0.69256 | Test loss: 0.68686\n",
      "Epoch: 70 | Loss: 0.69233 | Test loss: 0.68686\n",
      "Epoch: 71 | Loss: 0.69254 | Test loss: 0.68686\n",
      "Epoch: 72 | Loss: 0.69230 | Test loss: 0.68686\n",
      "Epoch: 73 | Loss: 0.69251 | Test loss: 0.68686\n",
      "Epoch: 74 | Loss: 0.69226 | Test loss: 0.68687\n",
      "Epoch: 75 | Loss: 0.69248 | Test loss: 0.68687\n",
      "Epoch: 76 | Loss: 0.69222 | Test loss: 0.68687\n",
      "Epoch: 77 | Loss: 0.69246 | Test loss: 0.68687\n",
      "Epoch: 78 | Loss: 0.69219 | Test loss: 0.68687\n",
      "Epoch: 79 | Loss: 0.69243 | Test loss: 0.68687\n",
      "Epoch: 80 | Loss: 0.69215 | Test loss: 0.68687\n",
      "Epoch: 81 | Loss: 0.69241 | Test loss: 0.68687\n",
      "Epoch: 82 | Loss: 0.69212 | Test loss: 0.68688\n",
      "Epoch: 83 | Loss: 0.69239 | Test loss: 0.68688\n",
      "Epoch: 84 | Loss: 0.69208 | Test loss: 0.68688\n",
      "Epoch: 85 | Loss: 0.69236 | Test loss: 0.68688\n",
      "Epoch: 86 | Loss: 0.69205 | Test loss: 0.68688\n",
      "Epoch: 87 | Loss: 0.69234 | Test loss: 0.68688\n",
      "Epoch: 88 | Loss: 0.69201 | Test loss: 0.68688\n",
      "Epoch: 89 | Loss: 0.69231 | Test loss: 0.68688\n",
      "Epoch: 90 | Loss: 0.69198 | Test loss: 0.68688\n",
      "Epoch: 91 | Loss: 0.69229 | Test loss: 0.68689\n",
      "Epoch: 92 | Loss: 0.69195 | Test loss: 0.68689\n",
      "Epoch: 93 | Loss: 0.69227 | Test loss: 0.68689\n",
      "Epoch: 94 | Loss: 0.69191 | Test loss: 0.68689\n",
      "Epoch: 95 | Loss: 0.69225 | Test loss: 0.68689\n",
      "Epoch: 96 | Loss: 0.69188 | Test loss: 0.68689\n",
      "Epoch: 97 | Loss: 0.69222 | Test loss: 0.68689\n",
      "Epoch: 98 | Loss: 0.69185 | Test loss: 0.68689\n",
      "Epoch: 99 | Loss: 0.69220 | Test loss: 0.68689\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "# Lists to store training loss for plotting\n",
    "train_loss_history = []\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    deep.train()\n",
    "\n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = deep(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))  # Turn logits -> pred probs -> pred labels\n",
    "\n",
    "    # 2. Calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train) \n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    deep.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Forward pass\n",
    "        test_logits = deep(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        \n",
    "        # 2. Calculate loss/accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "    \n",
    "    # Print out what's happening every epoch\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss.item():.5f} | Test loss: {test_loss.item():.5f}\")\n",
    "\n",
    "    # Append training loss to the list for plotting\n",
    "    train_loss_history.append(loss.item())\n",
    "\n",
    "    # Check for NaN or infinite values in tensors\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(\"Training loss is NaN or infinite. Exiting training loop.\")\n",
    "        break\n",
    "\n",
    "# Plot training loss over epochs\n",
    "plt.plot(range(epochs), train_loss_history, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 743.2023\n",
      "Epoch [20/100], Loss: 740.9802\n",
      "Epoch [30/100], Loss: 740.6160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Loss: 740.2402\n",
      "Epoch [50/100], Loss: 739.9799\n",
      "Epoch [60/100], Loss: 739.7703\n",
      "Epoch [70/100], Loss: 739.5802\n",
      "Epoch [80/100], Loss: 739.3683\n",
      "Epoch [90/100], Loss: 739.2172\n",
      "Epoch [100/100], Loss: 739.0782\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-56b9c6348174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtest_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Loss: {test_loss.item():.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = model(X_train).squeeze()\n",
    "    # Squeeze to remove extra `1` dimensions; this won't work unless model and data are on the same device \n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))  # Turn logits -> pred probs -> pred labels\n",
    "\n",
    "    loss = criterion(y_logits, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every few epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
